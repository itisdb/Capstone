{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voice LSTM",
      "provenance": [],
      "mount_file_id": "1Zup3PE-7b5noE-N0-MJA3uENSaaCN1pw",
      "authorship_tag": "ABX9TyPAWimNPWS020WnDTpUASSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itisdb/Capstone/blob/main/Voice_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1H2DMSrMrBD",
        "outputId": "f365751c-bed5-447c-a363-bf4d1f053e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone'...\n",
            "remote: Enumerating objects: 2985, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 2985 (delta 21), reused 66 (delta 18), pack-reused 2911\u001b[K\n",
            "Receiving objects: 100% (2985/2985), 195.92 MiB | 13.70 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (2904/2904), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/itisdb/Capstone.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Keras Dependency\n",
        "!sudo pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvFdzCb6Mwot",
        "outputId": "ee6fd3e6-6cf0-44da-9aef-fe57d7a60097"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.5.12.1.tar.gz (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▍                          | 10 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from np_utils) (0.16.0)\n",
            "Building wheels for collected packages: np-utils\n",
            "  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np-utils: filename=np_utils-0.5.12.1-py3-none-any.whl size=57131 sha256=1efd73077a2c782d6d8b588fa5f84cbfdb6e8c40b0d271b16b144bea9d7b2f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/4e/ef/095c24693723c329f4cdc1079861cdbb2487d4b41b2496a4e7\n",
            "Successfully built np-utils\n",
            "Installing collected packages: np-utils\n",
            "Successfully installed np-utils-0.5.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# text preprocessing\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# plots and metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# preparing input to our model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# keras layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense"
      ],
      "metadata": {
        "id": "sngmLdGRNgNp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining vector space dimension and fixed input size\n",
        "\n",
        "# Number of labels: joy, anger, fear, sadness, neutral\n",
        "num_classes = 5\n",
        "\n",
        "# Number of dimensions for word embedding\n",
        "embed_num_dims = 300\n",
        "\n",
        "# Max input length (max number of words) \n",
        "max_seq_len = 500\n",
        "\n",
        "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']"
      ],
      "metadata": {
        "id": "Ol76PqByNmCv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/Capstone/data/data_train.csv', encoding='utf-8')\n",
        "data_test = pd.read_csv('/content/Capstone/data/data_test.csv', encoding='utf-8')\n",
        "\n",
        "X_train = data_train.Text\n",
        "X_test = data_test.Text\n",
        "\n",
        "y_train = data_train.Emotion\n",
        "y_test = data_test.Emotion\n",
        "\n",
        "data = data_train.append(data_test, ignore_index=True)"
      ],
      "metadata": {
        "id": "iMNdlePwNoCL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.Emotion.value_counts())\n",
        "data.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "0OPcjq2JNuNH",
        "outputId": "ca77bee1-c248-423b-9fa5-e282b2620630"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joy        2326\n",
            "sadness    2317\n",
            "anger      2259\n",
            "neutral    2254\n",
            "fear       2171\n",
            "Name: Emotion, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>There are tons of other paintings that I thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Yet the dog had grown old and less capable , a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fear</td>\n",
              "      <td>When I get into the tube or the train without ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fear</td>\n",
              "      <td>This last may be a source of considerable disq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>anger</td>\n",
              "      <td>She disliked the intimacy he showed towards so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>sadness</td>\n",
              "      <td>When my family heard that my Mother's cousin w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Emotion                                               Text\n",
              "0  neutral   There are tons of other paintings that I thin...\n",
              "1  sadness  Yet the dog had grown old and less capable , a...\n",
              "2     fear  When I get into the tube or the train without ...\n",
              "3     fear  This last may be a source of considerable disq...\n",
              "4    anger  She disliked the intimacy he showed towards so...\n",
              "5  sadness  When my family heard that my Mother's cousin w..."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To input the data to our NN Model we'll need some preprocessing:\n",
        "\n",
        "#Tokenize our texts and count unique tokens\n",
        "#Padding: each input (sentence or text) has to be of the same lenght\n",
        "#Labels have to be converted to integeres and categorized\n",
        "\n",
        "def clean_text(data):\n",
        "    \n",
        "    # remove hashtags and @usernames\n",
        "    data = re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
        "    data = re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
        "    \n",
        "    # tekenization using nltk\n",
        "    data = word_tokenize(data)\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "SOcub2-TNv0k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using the NLTK Downloader to obtain the resource\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "texts = [' '.join(clean_text(text)) for text in data.Text]\n",
        "\n",
        "texts_train = [' '.join(clean_text(text)) for text in X_train]\n",
        "texts_test = [' '.join(clean_text(text)) for text in X_test]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fwIvEE1Nx4t",
        "outputId": "d56459ed-6489-4142-c2aa-879743df4db4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts_train[93])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEP7g1uENzvY",
        "outputId": "257d934b-56c2-494d-89ad-a9b1ca4fe29d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fantastic ! I did n't want to come back .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "\n",
        "# vacab size is number of unique words + reserved 0 index for padding\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "print('Number of unique words: {}'.format(len(index_of_words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2uSt3OsN189",
        "outputId": "aefe1dd9-e72c-47e4-db89-969b3c73399a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 12088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
        "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
        "\n",
        "X_train_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwKNnGSEN6Wu",
        "outputId": "b93f56c5-f4b7-4e98-cab9-139ece58b3d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,   119,    51,   345],\n",
              "       [    0,     0,     0, ...,    37,   277,   154],\n",
              "       [    0,     0,     0, ...,    16,     2,  1210],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,   876,     4,   909],\n",
              "       [    0,     0,     0, ...,     1,     6,   117],\n",
              "       [    0,     0,     0, ..., 10259,   173,    13]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorizing the labels\n",
        "\n",
        "encoding = {\n",
        "    'joy': 0,\n",
        "    'fear': 1,\n",
        "    'anger': 2,\n",
        "    'sadness': 3,\n",
        "    'neutral': 4\n",
        "}\n",
        "\n",
        "# Integer labels\n",
        "y_train = [encoding[x] for x in data_train.Emotion]\n",
        "y_test = [encoding[x] for x in data_test.Emotion]"
      ],
      "metadata": {
        "id": "HES6PMKKN8Sj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9XThv4qN-AR",
        "outputId": "5adb559a-9126-4ae9-bd42-a2b03c8dc3fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the pretrained words\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "BPn2io4SOAPs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can download and import any pre-trained word embeddings.\n",
        "#We will use 300 dimentional w2v pre-trained on wikipedia articles.\n",
        "#Free Resource : https://fasttext.cc/docs/en/english-vectors.html\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "    print('Downloading word vectors...')\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
        "                              'wiki-news-300d-1M.vec.zip')\n",
        "    print('Unzipping...')\n",
        "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('embeddings')\n",
        "    print('done.')\n",
        "    \n",
        "    os.remove('wiki-news-300d-1M.vec.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTUD9uMdOCD1",
        "outputId": "c4493f66-7815-486a-e7e7-5dcfc4c736cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading word vectors...\n",
            "Unzipping...\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
        "embedd_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XBc3WwTOElO",
        "outputId": "8bb29518-729c-4777-de7c-1275e891912c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12089, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect unseen words\n",
        "new_words = 0\n",
        "\n",
        "for word in index_of_words:\n",
        "    entry = embedd_matrix[index_of_words[word]]\n",
        "    if all(v == 0 for v in entry):\n",
        "        new_words = new_words + 1\n",
        "\n",
        "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
        "print('New words found: ' + str(new_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_edoVdwbOHQT",
        "outputId": "653eaa56-95c6-4f88-e9d6-4c49e3a01adb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words found in wiki vocab: 11442\n",
            "New words found: 646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create LSTM Pipeline\n"
      ],
      "metadata": {
        "id": "1pQyX4F6OT-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer before the actaul BLSTM \n",
        "embedd_layer = Embedding(vocab_size,\n",
        "                         embed_num_dims,\n",
        "                         input_length = max_seq_len,\n",
        "                         weights = [embedd_matrix],\n",
        "                         trainable=False)"
      ],
      "metadata": {
        "id": "aNHUeCn5OQyH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Pipeline\n"
      ],
      "metadata": {
        "id": "VQbrjXa0ObDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gru_output_size = 128\n",
        "bidirectional = True\n",
        "\n",
        "# Embedding Layer, LSTM or biLSTM, Dense, softmax\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "\n",
        "if bidirectional:\n",
        "    model.add(Bidirectional(GRU(units=gru_output_size,\n",
        "                              dropout=0.2,\n",
        "                              recurrent_dropout=0.2)))\n",
        "else:\n",
        "     model.add(GRU(units=gru_output_size,\n",
        "                dropout=0.2, \n",
        "                recurrent_dropout=0.2))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRQUMJasOXlv",
        "outputId": "d0f67302-36a4-4b2f-f962-0e2bc876dc20"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q70V3EAOdOo",
        "outputId": "97b028b7-4f25-4378-b42e-7fbcc3acc0d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 300)          3626700   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 256)              330240    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,958,225\n",
            "Trainable params: 331,525\n",
            "Non-trainable params: 3,626,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train the Model\n"
      ],
      "metadata": {
        "id": "JDMLPRGPOmPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "hist = model.fit(X_train_pad, y_train, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFiudgjvOfr7",
        "outputId": "46781533-724d-42c5-b65d-c78f93152eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "56/62 [==========================>...] - ETA: 29s - loss: 1.4238 - accuracy: 0.3790"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MzeI3SDWOn47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "Lv0ZxbgeOp5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]"
      ],
      "metadata": {
        "id": "QgsSEQP8OsOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(data_test.Emotion, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))"
      ],
      "metadata": {
        "id": "uO2CoqOvOx3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a HDF5 file 'my_model.h5'\n",
        "model.save('/content/drive/MyDrive/Capstone/models/biLSTM_w2v.h5')"
      ],
      "metadata": {
        "id": "HNKz9I69O4M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "predictor = load_model('/content/drive/MyDrive/Capstone/models/biLSTM_w2v.h5')"
      ],
      "metadata": {
        "id": "EK8iSzI7PDMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "metadata": {
        "id": "wKgZO_p4PEnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(data_test.Emotion, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mTbKU0TPF8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Message: {}\\nPredicted: {}'.format(X_test[4], predictions[4]))"
      ],
      "metadata": {
        "id": "FBQW4qIfPIDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "message = ['i have been really going through rough days']\n",
        "\n",
        "seq = tokenizer.texts_to_sequences(message)\n",
        "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
        "\n",
        "start_time = time.time()\n",
        "pred = model.predict(padded)\n",
        "\n",
        "print('Message: ' + str(message))\n",
        "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
      ],
      "metadata": {
        "id": "tDs4-DOMPLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voice Prediction:"
      ],
      "metadata": {
        "id": "3CzZ9-4VPROu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing all the packages required for speech input and recognition\n",
        "\n",
        "!sudo apt-get install portaudio19-dev python-pyaudio\n",
        "!sudo apt-get install python-gnuradio-audio-portaudio\n",
        "!pip install PyAudio\n",
        "!pip install SpeechRecognition\n",
        "!pip install pyttsx3\n",
        "!pip install wavio\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "MqPg7SeXPOWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports\n",
        "import numpy as np\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# js script needed to record input from colab\n",
        "RECORD = \"\"\"\n",
        "const sleep = time => new Promise(resolve => {\n",
        "setTimeout(resolve, time)\n",
        "}, )\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "const reader = new FileReader()\n",
        "reader.onloadend = e => resolve(e.srcElement.result)\n",
        "reader.readAsDataURL(blob)\n",
        "})\n",
        "var espacio = document.querySelector(\"#output-area\")\n",
        "var record = time => new Promise(async resolve => {\n",
        "stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "recorder = new MediaRecorder(stream)\n",
        "chunks = []\n",
        "recorder.ondataavailable = e => chunks.push(e.data)\n",
        "recorder.start()\n",
        "var numerillo = (time/1000)-1\n",
        "for (var i = 0; i < numerillo; i++) {\n",
        "espacio.appendChild(document.createTextNode(numerillo-i))\n",
        "await sleep(1000)\n",
        "espacio.removeChild(espacio.lastChild)\n",
        "}\n",
        "recorder.onstop = async ()=>{\n",
        "blob = new Blob(chunks)\n",
        "text = await b2text(blob)\n",
        "resolve(text)\n",
        "}\n",
        "recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "#record function that will record the audio for a default of 5 secs\n",
        "def record(sec=5):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  with open('audio.wav','wb') as f:\n",
        "    f.write(b)\n",
        "  return 'audio.wav'  # or webm ?\n",
        "\n",
        "\n",
        "#conversion of the .wav file into a readable format for speechrecognizer\n",
        "EXPECTED_SAMPLE_RATE = 16000\n",
        "\n",
        "def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):\n",
        "  audio = AudioSegment.from_file(user_file)\n",
        "  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)\n",
        "  audio.export(output_file, format=\"wav\")\n",
        "  return output_file"
      ],
      "metadata": {
        "id": "B4HuZR0mPS9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recognition of speech using SpeeechRecognition Package\n",
        "import speech_recognition as sr\n",
        "import time\n",
        "\n",
        "r = sr.Recognizer()\n",
        "n=int(input(\"Enter the number of seconds:\"))\n",
        "print(\"Speak Anything :\")\n",
        "print(\"Time Remaining:\")\n",
        "wave=record(n)\n",
        "converted_audio_file = convert_audio_for_model(wave)\n",
        "\n",
        "# obtain audio from the microphone\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile('/content/converted_audio_file.wav') as source:\n",
        "    audio = r.record(source) #read the wave file from the source\n",
        "\n",
        "# recognize speech using Google Speech Recognition\n",
        "try:\n",
        "    # for testing purposes, we're just using the default API key\n",
        "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
        "    # instead of `r.recognize_google(audio)`\n",
        "    text=r.recognize_google(audio)\n",
        "    print(\"You said:  \" + text)\n",
        "except sr.UnknownValueError:\n",
        "    print(\"AI could not understand audio\")\n",
        "except sr.RequestError as e:\n",
        "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
        "\n",
        "\n",
        "# Python code to convert string to list\n",
        "# We need it as input into the lstm pipeline is done with list and the output from the StT is in String  \n",
        "def Convert(string):\n",
        "    li = list(string.split(\" \"))\n",
        "    return li\n",
        "\n",
        "#Message encoding\n",
        "message = [' '.join(Convert(text))]\n",
        "\n",
        "seq = tokenizer.texts_to_sequences(message)\n",
        "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
        "\n",
        "start_time = time.time()\n",
        "pred = model.predict(padded)\n",
        "\n",
        "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
      ],
      "metadata": {
        "id": "BZHBVGAsPUWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8rEdpXCbPWSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}